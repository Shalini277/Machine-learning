---
title: "Predicting Net Promoter Score for MHE"


output:
  html_document:
    toc: true
    toc_depth: 2
    section_number: true
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## {.tabset}

### Overview & Data Clearning

<span style="text-decoration:underline">**Problem Statement**</span>

How to understand the customer concerns Via the NPS and improve certain area to give the best experience to the customers

<span style="text-decoration:underline">**Analysis**</span>

Analyze the following

1. How to improve the data quality and use it for NPS prediction

2. What are the significant factors influencing the detractors

3. What are the significant factors which contribute to improving the NPS

4. Find out the improvement opportunity within the depart using NPS score

<span style="text-decoration:underline">**Data clearing**</span> 

*Libraries used for RFM analysis*

```{r Libraries used}
library(caret)
library(car)
library(dplyr)
library(ggplot2)
library(fastDummies)
library(ROSE)
library(pROC)
```

***Import the complete data set to R***
```{r Data Import}
Complete_data<-read.csv("clipboard",sep = "\t",header = TRUE,stringsAsFactors = TRUE)
Hdata1<-Complete_data
```

***Data clearing***
```{r Data Processing}
colSums(is.na(Hdata1))
str(Hdata1)
Hdata1$NPS_Status<-as.factor(Hdata1$NPS_Status)
levels(Hdata1$NPS_Status)
Hdata1<-Hdata1[,-c(1,2,3,4,5,9:11,47,48,50)]
str(Hdata1)
```

### Logistic Regression

***Model using Logistic Regression***
```{r Logistic Regression}
LRModel1<-glm(NPS_Status~.,data=Hdata1,family = "binomial")
summary(LRModel1)
LRModel2<-step(LRModel1,trace = 0)
summary(LRModel2)
```

***Split the data into Train and Test for building the prediction model***
```{r Split the data }
set.seed(1234)
Index<-createDataPartition(Hdata1$NPS_Status, p=.80, list=FALSE)
Train<-Hdata1[Index,]
Test<-Hdata1[-Index,]
prop.table(table(Hdata1$NPS_Status))
table(Hdata1$NPS_Status)
prop.table(table(Train$NPS_Status))
table(Train$NPS_Status)
prop.table(table(Test$NPS_Status))
table(Test$NPS_Status)
```

***Build the prediction model using Train data***
```{r Train the Model}
LRModelP1<-glm(NPS_Status~.,data = Train, family="binomial")
summary(LRModelP1)
LRModelP2<-step(LRModelP1,trace=0)
summary(LRModelP2)
```

***Evaluate the model using the test data***
```{r cols.print=9, rows.print=20, cols.min.print=9}
Predicted<-predict(LRModelP2, newdata = Test, type="response")
Test$Predicted<-Predicted
Test$Class<-ifelse(Test$Predicted>=0.5,1,0)
Test$Actual<-ifelse(Test$NPS_Status=="Promotor",1,0)
Test$Class<-as.factor(Test$Class)
Test$Actual<-factor(Test$Actual)
Test
```

***Find the sensitivity and specificity***

```{r Sensitivity & Specificity}
confusionMatrix(Test$Class, Test$Actual,positive = "1")

```

***Check the Precision, rcall and accuracy to check the data imbalance***
```{r Data Imbalance}
accuracy.meas(Test$NPS_Status,Predicted)

```

***Find ROC and AUC***
```{r ROC and AUC}
par(pty="s")
roc(Test$Actual,Predicted, plot=TRUE, print.auc=TRUE,legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage",ylab="True Positive Percentage", col="#de2d26",lwd=4)
legend("bottomright", legend = "Logistic Regression", col="#de2d26",lwd=4)

```

***Create the data frame with TPP, FPP and Threshold***

```{r cols.print=9, rows.print=20, cols.min.print=9}
roc.info<-roc(Test$Actual,Predicted, legacy.axes=TRUE)
roc.df<-data.frame(TPP=roc.info$sensitivities*100,FPP=(1-roc.info$specificities)*100,Threshold=roc.info$thresholds)
roc.df

```

### Decesion Tree

***Libraries used for Decision Tree***
```{r Libraries used D}
library(caTools)
library(party)
library(magrittr)
```

***Import data into R***
```{r Import Data D Tree}
Ddata1<-Complete_data
Ddata1<-Ddata1[,-c(1,2,3,4,5,9:11,47,48,50)]
str(Ddata1)
Ddata1$NPS_Status<-as.factor(Ddata1$NPS_Status)
```

***Split the data into test and train***
```{r Split the Data}
set.seed(1234)
DIndex<-sample.split(Ddata1, SplitRatio = 0.8)
DTrain<-subset(Ddata1,DIndex==TRUE)
DTest<-subset(Ddata1,DIndex==FALSE)
table(DTrain$NPS_Status)
table(DTest$NPS_Status)
```

***Build the Model using train data***
```{r Build Model}
Dmodel<-ctree(NPS_Status~.,DTrain)
plot(Dmodel)
print(Dmodel)
```

***Evaluate the model using test data***
```{r cols.print=9, rows.print=20, cols.min.print=9}
PredictD<-predict(Dmodel,DTest)
DTest$Actual<-ifelse(DTest$NPS_Status=="Promotor",1,0)
DTest$Actual<-as.factor(DTest$Actual)
DTest$Predicted<-PredictD
DTest$Pre_Value<-ifelse(DTest$Predicted=="Promotor",1,0)
DTest$Predicted<-as.factor(DTest$Predicted)
DTest
```

***Create the confusion matrix***
```{r Confusion Matrix}
confusionMatrix(DTest$NPS_Status,PredictD)
```

***Compare ROC and AUC for logistic regression and Decision tree***
```{r Compare ROC and AUC}
par(pty="s")
roc(Test$Actual,Predicted, plot=TRUE, print.auc=TRUE,legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage",
    ylab="True Positive Percentage", col="#de2d26",lwd=4)
plot.roc(DTest$Actual, DTest$Pre_Value, percent=TRUE, col="#31a354", lwd=4, print.auc=TRUE, add=TRUE, print.auc.y=30)
legend("bottomright", legend = c("Logistic Regression", "Descision Tree"), col=c("#de2d26","#31a354"),lwd=4)
```

### Random Forest 

***Libraries used***
```{r Libraries used for RMF}
library(caTools)
library(party)
library(magrittr)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```


***Import data in R***
```{r Import R Data for RMF}
Rdata1<-Complete_data
Rdata1<-Rdata1[,-c(1,2,3,4,5,9:11,47,48,50)]
str(Rdata1)
Rdata1$NPS_Status<-as.factor(Rdata1$NPS_Status)
```

***Split the data into test and train***
```{r Split Data RFM}
set.seed(1234)
RIndex<-sample.split(Rdata1, SplitRatio = 0.8)
RTrain<-subset(Rdata1,RIndex==TRUE)
RTest<-subset(Rdata1,RIndex==FALSE)
```

***Create a Random Forest Model***
```{r Split Data}
t<-tuneRF(RTrain[,-32], RTrain[,32],
          stepfactor=0.5,
          plot=TRUE,
          ntreeTry=300,
          trace = TRUE,
          improve = 0.05)
RModel<-randomForest(NPS_Status~.,data=RTrain,
                     ntree=300,
                     mtry=13,
                     importance=TRUE,
                     proximity=TRUE)
RModel
```

***Evaluate the model using Test data***
```{r Evaluate RFM}
PredictR<-predict(RModel,RTest)
```

***Confusion Matrix***
```{r Confusion RFM}
confusionMatrix(PredictR,RTest$NPS_Status)
plot(RModel)
RTest$Predicted<-PredictR
RTest$Predicted_Value<-ifelse(RTest$Predicted=="Promotor",1,0)
RTest$Actual<-ifelse(RTest$NPS_Status=="Promotor",1,0)
```

***Which variable plays important role in the model***
```{r Variable Importance}
varImpPlot(RModel,
           Sort=T,
           n.var = 10,
           main = "Top 10 Important Variables")
```

***Compare ROC and AUC for Logistic Regression, Decision Tree, and Random Forest***
```{r ROC for RFM}
par(pty="s")
roc(Test$Actual,Predicted, plot=TRUE, print.auc=TRUE,legacy.axes=TRUE, percent=TRUE, xlab="False Positive Percentage",
    ylab="True Positive Percentage", col="#de2d26",lwd=4)
plot.roc(DTest$Actual, DTest$Pre_Value, percent=TRUE, col="#31a354", lwd=4, print.auc=TRUE,add=TRUE, print.auc.y=30)
plot.roc(RTest$NPS_Status,RTest$Predicted_Value,percent=TRUE, col="#756bb1", lwd=4, print.auc=TRUE,add=TRUE, print.auc.y=40)
legend("bottomright", legend = c("Logistic Regression", "Descision Tree", "Random Forest"), col=c("#de2d26","#31a354","#756bb1"),lwd=4)
```

### Inference
